_target_: models.titans_memory.TitansViTPredictor

# Standard ViT parameters
num_patches: 196  # 14x14 patches for 224x224 images
num_frames: 16    # number of frames in sequence
dim: 384          # embedding dimension
depth: 12         # number of transformer layers
heads: 8          # number of attention heads
mlp_dim: 1536     # MLP hidden dimension
dim_head: 64      # dimension per attention head
dropout: 0.1      # dropout rate
emb_dropout: 0.1  # embedding dropout rate
pool: 'cls'       # pooling method

# Titans memory parameters
memory_variant: "MAC"  # "MAC", "MAG", or "MAL"
d_k: 64              # key dimension for memory
d_v: 64              # value dimension for memory
n_persist: 16        # number of persistent tokens (MAC only)
window: 512          # window size for sliding attention (MAG only)

# Memory update parameters
alpha: 0.02          # forget rate
lr: 0.1              # inner-loop learning rate
beta: 0.9            # momentum rate
